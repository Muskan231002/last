{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Practical No. 13/14\n",
        "MNIST Handwritten Character Detection using PyTorch, Keras and Tensorflow."
      ],
      "metadata": {
        "id": "cO2YA0VlYY0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP3QL92FXsAi",
        "outputId": "d1f0a69f-a44f-4f02-e24d-396cc44ed584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 7ms/step - loss: 0.2953 - accuracy: 0.9143\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1437 - accuracy: 0.9570\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1094 - accuracy: 0.9675\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0893 - accuracy: 0.9724\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0766 - accuracy: 0.9765\n",
            "313/313 - 1s - loss: 0.0783 - accuracy: 0.9755 - 615ms/epoch - 2ms/step\n",
            "Test accuracy: 0.9754999876022339\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Define the Neural Network Architecture\n",
        "\n",
        "model = models.Sequential([\n",
        "\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(10)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Compile the Model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Train the Model\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch"
      ],
      "metadata": {
        "id": "Fs7UvYDyYUTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Define the Neural Network Architecture\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 128)\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.relu(self.conv1(x))\n",
        "\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        x = torch.relu(self.conv2(x))\n",
        "\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        x = x.view(-1, 1600)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "model = Net()\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Define Loss Function and Optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Train the Model\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 99:\n",
        "\n",
        "            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100}')\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cuc0zm2XtXF",
        "outputId": "63cd7575-af79-4635-be7d-d77120df7e56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16278652.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 489532.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4427082.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3494227.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch: 1, Batch: 100, Loss: 2.2472374510765074\n",
            "Epoch: 1, Batch: 200, Loss: 1.98066330909729\n",
            "Epoch: 1, Batch: 300, Loss: 1.1884246420860292\n",
            "Epoch: 1, Batch: 400, Loss: 0.603051301240921\n",
            "Epoch: 1, Batch: 500, Loss: 0.4363444474339485\n",
            "Epoch: 1, Batch: 600, Loss: 0.35312618777155874\n",
            "Epoch: 1, Batch: 700, Loss: 0.3015502916276455\n",
            "Epoch: 1, Batch: 800, Loss: 0.2572355356067419\n",
            "Epoch: 1, Batch: 900, Loss: 0.2653696473687887\n",
            "Epoch: 2, Batch: 100, Loss: 0.2347768970578909\n",
            "Epoch: 2, Batch: 200, Loss: 0.20832354824990035\n",
            "Epoch: 2, Batch: 300, Loss: 0.20466944746673107\n",
            "Epoch: 2, Batch: 400, Loss: 0.1684703254327178\n",
            "Epoch: 2, Batch: 500, Loss: 0.16814141873270272\n",
            "Epoch: 2, Batch: 600, Loss: 0.14401419542729854\n",
            "Epoch: 2, Batch: 700, Loss: 0.15841300997883082\n",
            "Epoch: 2, Batch: 800, Loss: 0.14627151366323232\n",
            "Epoch: 2, Batch: 900, Loss: 0.1458158240094781\n",
            "Epoch: 3, Batch: 100, Loss: 0.1207556202635169\n",
            "Epoch: 3, Batch: 200, Loss: 0.1312773783877492\n",
            "Epoch: 3, Batch: 300, Loss: 0.1310655278339982\n",
            "Epoch: 3, Batch: 400, Loss: 0.1184741917438805\n",
            "Epoch: 3, Batch: 500, Loss: 0.11488929778337478\n",
            "Epoch: 3, Batch: 600, Loss: 0.11497406832873822\n",
            "Epoch: 3, Batch: 700, Loss: 0.10627795157954097\n",
            "Epoch: 3, Batch: 800, Loss: 0.10850151512771845\n",
            "Epoch: 3, Batch: 900, Loss: 0.09998563166707754\n",
            "Epoch: 4, Batch: 100, Loss: 0.10156564645469189\n",
            "Epoch: 4, Batch: 200, Loss: 0.10658395871520042\n",
            "Epoch: 4, Batch: 300, Loss: 0.09458498250693083\n",
            "Epoch: 4, Batch: 400, Loss: 0.08971971117891371\n",
            "Epoch: 4, Batch: 500, Loss: 0.09105670765042305\n",
            "Epoch: 4, Batch: 600, Loss: 0.09148086472414434\n",
            "Epoch: 4, Batch: 700, Loss: 0.08599968103691935\n",
            "Epoch: 4, Batch: 800, Loss: 0.07733684133738279\n",
            "Epoch: 4, Batch: 900, Loss: 0.08378330901265145\n",
            "Epoch: 5, Batch: 100, Loss: 0.07724336777813733\n",
            "Epoch: 5, Batch: 200, Loss: 0.07579496517777443\n",
            "Epoch: 5, Batch: 300, Loss: 0.0748196264822036\n",
            "Epoch: 5, Batch: 400, Loss: 0.08355472523719072\n",
            "Epoch: 5, Batch: 500, Loss: 0.08062376949936151\n",
            "Epoch: 5, Batch: 600, Loss: 0.07891837515402585\n",
            "Epoch: 5, Batch: 700, Loss: 0.07254710246808826\n",
            "Epoch: 5, Batch: 800, Loss: 0.07682707500644029\n",
            "Epoch: 5, Batch: 900, Loss: 0.07593207824975252\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31t6f_auYI-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}